{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def get_paths(input_folder):\n",
    "    \"\"\"\n",
    "    Get a list of all .json file paths in the input folder.\n",
    "    :param input_folder: Folder containing .json files.\n",
    "    :return: List of file paths.\n",
    "    \"\"\"\n",
    "    list_files = glob.glob(input_folder + '/*.json')\n",
    "    return list_files\n",
    "\n",
    "def load_text(json_path):\n",
    "    \"\"\"\n",
    "    Load and parse JSON content from a given file.\n",
    "    :param json_path: Path to the JSON file.\n",
    "    :return: Parsed JSON content.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data = json_file.read()\n",
    "        content = json.loads(data)\n",
    "    \n",
    "    return content\n",
    "\n",
    "def process_and_write(loaded_dicts, output_folder, json_path):\n",
    "    \"\"\"\n",
    "    Process the loaded JSON data and write it to a .conll formatted file.\n",
    "    :param loaded_dicts: Parsed content of the JSON file.\n",
    "    :param output_folder: Folder to save the .conll files.\n",
    "    :param json_path: Path of the original JSON file.\n",
    "    \"\"\"\n",
    "    # Ensure output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Create the output file name with .conll extension\n",
    "    base_name = os.path.basename(json_path)[:-5]  # Remove .json extension\n",
    "    conll_filename = base_name + '.txt'\n",
    "    output_path = os.path.join(output_folder, conll_filename)\n",
    "    \n",
    "    # Write to the output file in CoNLL format\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for item in loaded_dicts:\n",
    "            for annotation in item.get('annotations', []):\n",
    "                for result in annotation.get('result', []):\n",
    "                    value = result.get('value', {})\n",
    "                    text = value.get('text', '')\n",
    "                    label = ','.join(value.get('hypertextlabels', []))\n",
    "                    if text and label:\n",
    "                        f.write(f\"{text} {label}\\n\")\n",
    "                f.write(\"\\n\")  # Separate different annotations with a new line\n",
    "\n",
    "def convert_json_to_conll(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Main function to convert all JSON files in the input folder to CoNLL format files.\n",
    "    :param input_folder: Folder containing .json files.\n",
    "    :param output_folder: Folder where .conll files will be saved.\n",
    "    \"\"\"\n",
    "    json_paths = get_paths(input_folder)\n",
    "    \n",
    "    for json_path in json_paths:\n",
    "        loaded_dicts = load_text(json_path)\n",
    "        process_and_write(loaded_dicts, output_folder, json_path)\n",
    "\n",
    "# Specify the input folder containing JSON files and the output folder for CoNLL files\n",
    "input_folder = r'C:\\Users\\archishman vb\\OneDrive\\Desktop\\annotated\\parties information'\n",
    "output_folder = r'C:\\Users\\archishman vb\\OneDrive\\Desktop\\annotated\\parties_info_data'\n",
    "\n",
    "# Convert all JSON files in the input folder to CoNLL format\n",
    "convert_json_to_conll(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(loaded_dicts, output_folder, json_path):\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Create the output file name with .conll extension\n",
    "    base_name = os.path.basename(json_path)[:-5]  # Remove .json extension\n",
    "    conll_filename = base_name + '.txt'\n",
    "    output_path = os.path.join(output_folder, conll_filename)\n",
    "    \n",
    "    # Write to the output file in CoNLL format\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for item in loaded_dicts:\n",
    "            for annotation in item.get('annotations', []):\n",
    "                for result in annotation.get('result', []):\n",
    "                    value = result.get('value', {})\n",
    "                    text = value.get('text', '')\n",
    "                    label = ','.join(value.get('hypertextlabels', []))\n",
    "                    if text and label:\n",
    "                        f.write(f\"{text} \\n\")\n",
    "                f.write(\"\\n\")  # Separate different annotations with a new line\n",
    "\n",
    "def convert_json_to_text(input_folder, output_folder):\n",
    "    json_paths = get_paths(input_folder)\n",
    "    \n",
    "    for json_path in json_paths:\n",
    "        loaded_dicts = load_text(json_path)\n",
    "        process(loaded_dicts, output_folder, json_path)\n",
    "\n",
    "# Specify the input folder containing JSON files and the output folder for CoNLL files\n",
    "input_folder = r'C:\\Users\\archishman vb\\OneDrive\\Desktop\\annotated\\parties information'\n",
    "output_folder = r'C:\\Users\\archishman vb\\OneDrive\\Desktop\\annotated\\parties_info_text'\n",
    "\n",
    "# Convert all JSON files in the input folder to CoNLL format\n",
    "convert_json_to_text(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u5168' in position 32: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 96\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;66;03m# Write the predicted entity-label pairs to the output file\u001b[39;00m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m entity, label \u001b[38;5;129;01min\u001b[39;00m predicted_entities:\n\u001b[1;32m---> 96\u001b[0m             \u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mentity\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlabel\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Calculate F1 score using the ground truth file\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_f1_score\u001b[39m(predictions_file, ground_truth_file):\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u5168' in position 32: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "from together import Together\n",
    "\n",
    "# Define the one-shot example for each entity label\n",
    "one_shot_prompt = \"\"\"\n",
    "You are an AI tasked with identifying specific entities in a text. Below is an example for each entity type with labeled entities, followed by a new text where you will need to identify entities based on the example.\n",
    "\n",
    "Example:\n",
    "Input: \"BRIGHT HORIZONS CAPITAL CORP\"\n",
    "Output: Organization Name\n",
    "\n",
    "Input: \"Swing Line Lender\"\n",
    "Output: Organization Role\n",
    "\n",
    "Input: \"legal counsel\"\n",
    "Output: Organization Sub-Role\n",
    "\n",
    "Input: \"Don R. Madison\"\n",
    "Output: Person Name\n",
    "\n",
    "Input: \"Senior Vice President\"\n",
    "Output: Person Position\n",
    "\n",
    "Input: \"40 Wantage Avenue, Branchville, New Jersey 07890\"\n",
    "Output: Location\n",
    "\n",
    "Input: \"corporate headquarters\"\n",
    "Output: Location Type\n",
    "\n",
    "New Text:\n",
    "{input_text}\n",
    "\n",
    "Please extract the entities from the 'New Text' and label them in the following format:\n",
    "- \"Entity\" \"Label\"\n",
    "\n",
    "Labels:\n",
    "· Organization Name\n",
    "· Organization Role\n",
    "· Organization Sub-Role\n",
    "· Location\n",
    "· Location Type\n",
    "· Person Name\n",
    "· Person Position\n",
    "\"\"\"\n",
    "\n",
    "# Read the text file where each line is a text to be labeled\n",
    "input_file_path = r'C:\\Users\\archishman vb\\OneDrive\\Desktop\\annotated\\parties_info_text\\test.txt'\n",
    "output_file_path = r'C:\\Users\\archishman vb\\OneDrive\\Desktop\\predicted_labels.txt'\n",
    "ground_truth_file_path = r'C:\\Users\\archishman vb\\OneDrive\\Desktop\\annotated\\parties_info_data\\ground_truth.txt'\n",
    "\n",
    "# Initialize the Together API client\n",
    "client = Together(api_key='961df14a57ad71c7ec591c73955e59825b7b8c57b7d6dea750bceee02fed625b')\n",
    "\n",
    "# Open the input file and read the content\n",
    "with open(input_file_path, 'r') as input_file, open(output_file_path, 'w') as output_file:\n",
    "    for line in input_file:\n",
    "        text_content = line.strip()\n",
    "        \n",
    "        # Construct the full prompt by inserting the current line into the one-shot example\n",
    "        final_prompt = one_shot_prompt.format(input_text=text_content)\n",
    "        \n",
    "        # Get predictions from the LLM (without streaming)\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "            max_tokens=2048,\n",
    "            temperature=0.7,\n",
    "            top_p=0.7,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1,\n",
    "            stop=[\"<|eot_id|>\", \"<|eom_id|>\"]\n",
    "        )\n",
    "\n",
    "        # Extract the LLM's output\n",
    "        predicted_output = response.choices[0].message.content\n",
    "\n",
    "        # Define a function to parse the predicted entities\n",
    "        def parse_entities(predicted_output):\n",
    "            entities = []\n",
    "            lines = predicted_output.strip().split(\"\\n\")\n",
    "            for line in lines:\n",
    "                match = re.match(r'(.+?) \"(.+)\"', line)\n",
    "                if match:\n",
    "                    entity, label = match.groups()\n",
    "                    entities.append((entity.strip(), label.strip()))\n",
    "            return entities\n",
    "\n",
    "        # Parse the predicted entities\n",
    "        predicted_entities = parse_entities(predicted_output)\n",
    "        \n",
    "        # Write the predicted entity-label pairs to the output file\n",
    "        for entity, label in predicted_entities:\n",
    "            output_file.write(f\"{entity} {label}\\n\")\n",
    "\n",
    "# Calculate F1 score using the ground truth file\n",
    "def calculate_f1_score(predictions_file, ground_truth_file):\n",
    "    with open(predictions_file, 'r') as pred_file, open(ground_truth_file, 'r') as gt_file:\n",
    "        predicted_labels = []\n",
    "        true_labels = []\n",
    "        \n",
    "        # Read predicted labels\n",
    "        for line in pred_file:\n",
    "            _, label = line.strip().rsplit(' ', 1)\n",
    "            predicted_labels.append(label)\n",
    "        \n",
    "        # Read true labels\n",
    "        for line in gt_file:\n",
    "            _, label = line.strip().rsplit(' ', 1)\n",
    "            true_labels.append(label)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        return f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "# Path to the predicted and ground truth files\n",
    "f1 = calculate_f1_score(output_file_path, ground_truth_file_path)\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
